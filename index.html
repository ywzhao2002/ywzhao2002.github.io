<!DOCTYPE html>
<html lang="en">

<head>
    <title>Yunwei Jacob Zhao </title>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta property="og:title" content="Yunwei Zhao" />
    <meta property="og:image" content="https://ywzhao.github.io/img/Jacob8.png" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="author" content="Yunwei Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="shortcut icon" type="img/Jacob8.png" href="favicon.ico" />

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</head>

<body>
    <style>
        pre {
            text-align: left;
            white-space: pre-line;
        }
    </style>
    <div class="container mt-5">
        <div class="row mb-3">
            <div class="col">
                <h1>Yunwei Jacob Zhao </h1>
            </div>
        </div>
        <div class="row">
            <div class="col-md-3 order-md-2">
                <img src="img/Jacob8.png" alt="Jacob" class="img-fluid rounded">
            </div>
            <div class="col-md-8 order-md-1">
                <p>
                    Hi! I am a second year undergraduate majoring in Computer Science at University of Washington,
                    Seattle.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col">
                <p>
                    Email: yzhao4 [<a href="https://en.wikipedia.org/wiki/At_sign" target="_blank">at</a>] uw.edu
                </p>
                <p>
                    Links:
                    [<a href="" target="_blank">Full CV</a>] [<a href="https://twitter.com/zhao_yunwei"
                        target="_blank">Twitter</a>] [<a href="https://github.com/JacobYunweiZhao"
                        target="_blank">Github</a>]
                </p>
            </div>
        </div>

        <!--             <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                    	<li>
                            (9/2019) Happy Senior Year !!!
                        </li>
                        <li>
                            (8/2019) Paper on Specializing Word Embeddings (for Parsing) 
                            by Information Bottleneck, accepted to EMNLP 2019.
                        </li>
                        <li>
                            (6/2019) I'm starting a summer intern at Harvardnlp, working on 
                            controllability and interpretability of autoregressive neural 
                            network.
                        </li>
                        <li>
                            (2/2019) Paper on Generative Model of Punctuation
                            for Dependency Parsing, accepted to TACL 2019.
                        </li>
                    </ul>
                </div>
            </div> -->

        <hr>
        <div class="row" id="publications">
            <div class="col">
                <h2>Publications</h2>
                <!-- <ul>
                    <li>

                        <a href="pdf/prefix_tuning.pdf" target="_blank">
                            <b>Prefix-Tuning: Optimizing Continuous Prompts for Generation</b>
                        </a>
                        <br />
                        <b>Xiang Lisa Li</b>
                        and <a href="https://cs.stanford.edu/~pliang" target="_blank">Percy Liang</a>

                        <br />
                        In <a href="" target="_blank">
                            <b>
                                Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
                                and the 11th International Joint Conference on Natural Language Processing (Volume 1:
                                Long Papers) </b></a>
                        <br />



                        [<a href="#" onclick="$('#acl2021_bib').toggle();return false;">bib</a>]
                        [<a href="#" onclick="$('#acl2021_abstract').toggle();return false;">abstract</a>]
                        <!-- [<a href="" target="_blank">dataset</a>]
                        <div id="acl2021_abstract" class="abstract" style="display:none;">
                            <p>
                                Fine-tuning is the de facto way of leveraging large pretrained language models for
                                downstream tasks. However, fine-tuning modifies all the language model parameters and
                                therefore necessitates storing a full copy for each task. In this paper, we propose
                                prefix-tuning, a lightweight alternative to fine-tuning for natural language generation
                                tasks, which keeps language model parameters frozen and instead optimizes a sequence of
                                continuous task-specific vectors, which we call the prefix. Prefix-tuning draws
                                inspiration from prompting for language models, allowing subsequent tokens to attend to
                                this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for
                                table-to-text generation and to BART for summarization. We show that by learning only
                                0.1% of the parameters, prefix-tuning obtains comparable performance in the full data
                                setting, outperforms fine-tuning in low-data settings, and extrapolates better to
                                examples with topics that are unseen during training.
                            </p>
                        </div>
                        <div id="acl2021_bib" class="bib" style="display:none;">
                            <pre>
                                @inproceedings{li-liang-2021-prefix,
                                    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
                                    author = "Li, Xiang Lisa  and
                                      Liang, Percy",
                                    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
                                    month = aug,
                                    year = "2021",
                                    address = "Online",
                                    publisher = "Association for Computational Linguistics",
                                    url = "https://aclanthology.org/2021.acl-long.353",
                                    doi = "10.18653/v1/2021.acl-long.353",
                                    pages = "4582--4597",
                                }
                                </pre>
                        </div>
                    </li>
                    <br />


                    <!-- <a href="pdf/control_gen.pdf" target="_blank">
                        <b>Posterior Control of Blackbox Generation</b>
                    </a>
                    <br />
                    <b>Xiang Lisa Li</b>
                    and <a href="http://rush-nlp.com" target="_blank">Alexander Rush</a>

                    <br />
                    In <a href="https://acl2020.org" target="_blank">
                        <b>
                            Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
                            (ACL) </b></a>, 2020.
                    <br /> 



                    [<a href="#" onclick="$('#acl2020_bib').toggle();return false;">bib</a>]
                    [<a href="#" onclick="$('#acl2020_abstract').toggle();return false;">abstract</a>]
                    [<a href="pdf/control_gen_app.pdf" target="_blank">appendix</a>]
                    <!-- [<a href="" target="_blank">dataset</a>]
                    <div id="acl2020_abstract" class="abstract" style="display:none;">
                        <p>
                            Text generation often requires high-precision output that obeys task-specific rules. This
                            fine-grained control is difficult to enforce with off-the-shelf deep learning models. In
                            this work, we consider augmenting neural generation models with discrete control states
                            learned through a structured latent-variable approach. Under this formulation, task-specific
                            knowledge can be encoded through a range of rich, posterior constraints that are effectively
                            trained into the model. This approach allows users to ground internal model decisions based
                            on prior knowledge, without sacrificing the representational power of neural generative
                            models. Experiments consider applications of this approach for text generation. We find that
                            this method improves over standard benchmarks, while also providing fine-grained control.
                        </p>
                    </div>
                    <div id="acl2020_bib" class="bib" style="display:none;">
                        <pre>
                                @inproceedings{li-rush-2020,
                                  author =      {Xiang Lisa Li and Alexander M. Rush},
                                  title =       {Posterior Control of Blackbox Generation},
                                  booktitle =   {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
                                  year =        {2020},
                                  month =       jul,
                                  address =     {Online},
                                  url =         {<a href="https://xiangli1999.github.io/pdf/control_gen.pdf">https://xiangli1999.github.io/pdf/control_gen.pdf</a>}
                                }
                                </pre>
                    </div>
                    </li>
                    <br />



                    <li>
                        <a href="pdf/VIB.pdf" target="_blank">
                            <b>Specializing Word Embeddings (for Parsing) by Information Bottleneck</b>
                        </a>
                        <br />
                        <b>Xiang Lisa Li</b>
                        and <a href="http://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>
                        <br />
                        In <a href="https://www.emnlp-ijcnlp2019.org" target="_blank">
                            <b>
                                Conference on Empirical Methods in Natural Language Processing
                                (EMNLP-IJCNLP)</b></a>, 2019.
                        <br />

                        <b>Best Paper Award at EMNLP-IJCNLP 2019</b>
                        <br />


                        [<a href="#" onclick="$('#emnlp2019_bib').toggle();return false;">bib</a>]
                        [<a href="#" onclick="$('#emnlp2019_abstract').toggle();return false;">abstract</a>]
                        [<a href="pdf/VIB-supp.pdf" target="_blank">appendix</a>]
                        <!-- [<a href="" target="_blank">dataset</a>]
                        <div id="emnlp2019_abstract" class="abstract" style="display:none;">
                            <p>
                                Pre-trained word embeddings like ELMo and BERT contain rich syntactic
                                and semantic information, resulting in state-of-the-art performance on
                                various tasks. We propose a very fast variational information bottleneck
                                (VIB) method to nonlinearly compress these embeddings, keeping only the
                                information that helps a discriminative parser. We compress each word
                                embedding to either a discrete tag or a continuous vector.
                                In the discrete version, our automatically compressed tags form an
                                alternative tag set: we show experimentally that our tags capture most
                                of the information in traditional POS tag annotations, but our tag
                                sequences can be parsed more accurately at the same level of tag granularity.
                                In the continuous version, we show experimentally that moderately compressing
                                the word embeddings by our method yields a more accurate parser in 8 of 9
                                languages, unlike simple dimensionality reduction.
                            </p>
                        </div>
                        <div id="emnlp2019_bib" class="bib" style="display:none;">
                            <pre>
								@inproceedings{li-eisner-2019,
								  author =      {Xiang Lisa Li and Jason Eisner},
								  title =       {Specializing Word Embeddings (for Parsing) by
								                 Information Bottleneck},
								  booktitle =   {Proceedings of the 2019 Conference on Empirical
								                 Methods in Natural Language Processing and 9th
								                 International Joint Conference on Natural Language
								                 Processing},
								  year =        {2019},
								  month =       nov,
								  address =     {Hong Kong},
								  url =         {<a href="http://cs.jhu.edu/~jason/papers/#li-eisner-2019">http://cs.jhu.edu/~jason/papers/#li-eisner-2019</a>}
								}
								</pre>
                        </div>
                    </li>
                    <br />
                    <li>
                        <a href="pdf/punctuation.pdf" target="_blank">
                            <b>A Generative Model for Punctuation in Dependency Trees</b>
                        </a>
                        <br />
                        <b>Xiang Lisa Li </b> and
                        <a href="http://www.cs.jhu.edu/~wdd/" target="_blank">Dingquan Wang</a> and
                        <a href="http://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>.
                        <br />
                        In <a href="https://www.transacl.org/ojs/index.php/tacl" target="_blank">
                            <b> Transactions of the Association for Computational
                                Linguistics (TACL)</b></a>, 2019.
                        <br />
                        [<a href="#" onclick="$('#tacl2019_bib').toggle();return false;">bib</a>]
                        [<a href="#" onclick="$('#tacl2019_abstract').toggle();return false;">abstract</a>]
                        [<a href="https://arxiv.org/abs/1906.11298" target="_blank">arxiv</a>]

                        <div id="tacl2019_abstract" class="abstract" style="display:none;">
                            <p>
                                Treebanks traditionally treat punctuation marks as ordinary words, but linguists have
                                suggested that a tree’s “true” punctuation marks are not observed (Nunberg, 1990). These
                                latent “underlying” marks serve to delimit or separate constituents in the syntax tree.
                                When the tree’s yield is rendered as a written sentence, a string rewriting mechanism
                                transduces the underlying marks into “surface” marks, which are part of the observed
                                (surface) string but should not be regarded as part of the tree. We formalize this idea
                                in a generative model of punctuation that admits efficient dynamic programming. We train
                                it without observing the underlying marks, by locally maximizing the incomplete data
                                likelihood (similarly to EM). When we use the trained model to reconstruct the tree’s
                                underlying punctuation, the results appear plausible across 5 languages, and in
                                particular, are consistent with Nunberg’s analysis of English. We show that our
                                generative model can be used to beat baselines on punctuation restoration. Also, our
                                reconstruction of a sentence’s underlying punctuation lets us appropriately render the
                                surface punctuation (via our trained underlying-to-surface mechanism) when we
                                syntactically transform the sentence.
                            </p>
                        </div>

                        <div id="tacl2019_bib" class="bib" style="display:none;">
                            <pre>
								@inproceedings{li-eisner-2019,
								  author =      {Xiang Lisa Li and Jason Eisner},
								  title =       {Specializing Word Embeddings (for Parsing) by
								                 Information Bottleneck},
								  booktitle =   {Proceedings of the 2019 Conference on Empirical
								                 Methods in Natural Language Processing and 9th
								                 International Joint Conference on Natural Language
								                 Processing},
								  year =        {2019},
								  month =       nov,
								  address =     {Hong Kong},
								  url =         {<a href="http://cs.jhu.edu/~jason/papers/#li-eisner-2019">http://cs.jhu.edu/~jason/papers/#li-eisner-2019</a>}
								}
								</pre>
                        </div>

                    </li>
                </ul>
                <br />
                <h5>Older publications on Medical Imaging:</h5>
                <ul>
                    <li>
                        <a href="http://dx.doi.org/10.1007/978-3-030-02628-8_9" target="_blank">
                            <b>Shortcomings of Ventricle Segmentation Using Deep Convolutional Networks</b>
                        </a>
                        <br />
                        Muhan Shao, Shuo Han, Aaron Carass, <b>Xiang Li</b>, Ari M. Blitz, Jerry L. Prince, Lotta M.
                        Ellingsen
                        <br />
                        In
                        <b> Deep Learning Fails Workshop in conjunction with the 21st International Conference on
                            MICCAI</b>, 2018.
                        <br />

                    </li>
                    <br />

                    <li>
                        <a href="http://dx.doi.org/10.1117/12.2295613" target="_blank">
                            <b>Multi-atlas Segmentation of the Hydrocephalus Brain Using an Adaptive Ventricle Atlas</b>
                        </a>
                        <br />
                        Muhan Shao, Aaron Carass, <b>Xiang Li</b>, Blake E. Dewey, Ari M. Blitz, Jerry L. Prince, Lotta
                        M. Ellingsen
                        <br />
                        In
                        <b>Proceedings of SPIE Medical Imaging</b>, 2018.
                        <br />
                    </li>
                    <br />

                    <li>
                        <a href="http://dx.doi.org/10.1117/12.2293633" target="_blank">
                            <b>Deformable Model Reconstruction of the Subarachnoid Space</b>
                        </a>
                        <br />
                        Jeffrey Glaister, Muhan Shao, <b>Xiang Li</b>, Aaron Carass, Snehashis Roy, Ari M. Blitz, Jerry
                        L. Prince, Lotta M. Ellingsen
                        <br />
                        In
                        <b>Proceedings of SPIE Medical Imaging</b>, 2018.
                        <br />
                    </li>
                    <br />

                    <li>
                        <a href="http://dx.doi.org/10.1007/978-3-319-67434-6_3" target="_blank">
                            <b>Whole Brain Parcellation with Pathology: Validation on Ventriculomegaly Patients</b>
                        </a>
                        <br />
                        Aaron Carass, Muhan Shao, <b>Xiang Li</b>, Blake E. Dewey, Ari M. Blitz, Snehashis Roy, Dzung L.
                        Pham, Jerry L. Prince, Lotta M. Ellingsen
                        <br />
                        In
                        <b>Workshop on Patch-based Techniques in Medical Imaging, MICCAI</b>, 2017.
                        <br />
                    </li>



                </ul> -->
            </div>
        </div>
        <hr>
        <div class="row">
            <div class="col">
                <h2>Honors & Awards</h2>
                <ul>
                    <!--<li>
                        (Sep. 2020)
                        Stanford Graduate Fellowship
                        <br />
                    </li>

                    <li>
                        (May. 2020)
                        Outstanding Senior Award
                        <br />
                    </li>

                    <li>
                        (Dec. 2019)
                        <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/"
                            target="_blank"> Outstanding Undergraduate Researcher Award </a> (Computing Research
                        Association)
                    </li>
                    <li>
                        (Nov. 2019)
                        Best Paper Award at EMNLP-IJCNLP
                        <br />
                    </li>

                    <li>
                        (Aug. 2019)
                        <a href="https://www.cs.jhu.edu/masson-fellowship/" target="_blank">Gerald M. Masson
                            Fellowship</a>
                        <br />
                    </li>
                    <li>
                        (May. 2019)
                        <a href="https://www.cs.jhu.edu/2019/05/14/2019-convocation-and-department-awardees/#.XWYJ7pMzZ0I"
                            target="_blank">
                            Michael J.Muuss Research Award </a>
                        <br />
                    </li>
                    <li>
                        (April. 2019)
                        <a href="https://twitter.com/DCDataFest/status/1115054593740812295" target="_blank">
                            Best Insight AND Best Visualization AND Best Use of Outside Data Award</a>

                    </li>
                    <li>
                        (Nov. 2018)
                        <a href="https://research.jhu.edu/hour/internal/pura/" target="_blank">
                            Provost’s Undergraduate Research Award (PURA)</a>
                    </li>
                    <li>
                        (May. 2018)
                        Research Experience for Undergraduate (REU) Fellowship
                    </li>
                    <li>
                        (May. 2018) Fellowship William Huggins Summer Fellowship
                    </li>
                    <li>
                        (May. 2017) Summer Training and Research (STAR) Fellowship
                    </li>
                    <li>
                        (Nov. 2018 - Present) Member of Tau Beta Pi
                    </li>
                    <li>
                        (April 2019 - Present) Member of Upsilon Pi Epsilon
                    </li> -->
                    <li>
                        (Sep. 2019 - Present) Dean's List
                    </li>
                </ul>
            </div>
        </div>

        <hr>
        <div class="row">
            <div class="col">
                <h2>Teaching Experience</h2>
                <ul>
                    <!-- <li>
                        (Spring 2020) TA @ Introduction to Statistics (AMS 553.430/630)
                    </li>
                    <li>
                        (Fall 2019) CA @ Natural Language Processing (CS 520.465/665)
                    </li>
                    <li>
                        (Spring 2019) TA @ Introduction to Probability (AMS 553.420/620)
                    </li>
                    <li>
                        (Fall 2018) CA @ Natural Language Processing (CS 520.465/665)
                    </li>
                    <li>
                        (Fall 2018) TA @ Introduction to Probability (AMS 553.420/620)
                    </li>
                    <li>
                        (Spring 2017) TA @ Introduction to Probability (AMS 553.420/620)
                    </li>
                    <li>
                        (Fall 2017) TA @ Introduction to Probability (AMS 553.420/620)
                    </li>

                    <small>
                        Interestingly, a perpetual prob TA is switching to stats... Hope we can have fun in 430 :)
                    </small> -->

                </ul>
            </div>
        </div>

        <footer class="pt-2 my-md-2 pt-md-2 border-top">
            <div class="row justify-content-center">
                <div class="col-6 col-md text-left align-self-center">
                    <p class="h5 text-muted">
                        Jacob, 2021
                    </p>
                </div>
            </div>
        </footer>
    </div>
    <!-- <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'UA-51640218-1', 'auto');
        ga('send', 'pageview');
    </script> -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LEC0YQ8DG8"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-LEC0YQ8DG8');
    </script>
</body>

</html>